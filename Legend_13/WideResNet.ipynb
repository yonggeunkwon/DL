{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1pLXm3_uYEmHiXOy6D8Lvgm0lHY3PqWA_","timestamp":1676227781354}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dp5DfpI0uHuW"},"outputs":[],"source":["import torch\n","from torch import nn"]},{"cell_type":"code","source":["class WiderBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, projection=None, drop_p=0.3):\n","        # drop_p = 0.3 for CIFAR, 0.4 for SVHN\n","        super().__init__()\n","\n","        self.residual = nn.Sequential(nn.BatchNorm2d(in_channels),\n","                                      nn.ReLU(inplace=True),\n","                                      nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias = False),\n","                                      nn.BatchNorm2d(out_channels),\n","                                      nn.ReLU(inplace=True),\n","                                      nn.Dropout(drop_p),\n","                                      nn.Conv2d(out_channels, out_channels, 3, padding=1, bias = False))\n","\n","        self.projection = projection\n","\n","    def forward(self, x):\n","\n","        residual = self.residual(x)\n","\n","        if self.projection is not None:\n","            shortcut = self.projection(x)\n","        else:\n","            shortcut = x\n","\n","        out = residual + shortcut # 엉! ReLU 였는데 ReLU 없음!\n","        return out\n","\n","class WRN(nn.Module):\n","    def __init__(self, depth, k, num_classes=1000, init_weights=True):\n","        super().__init__()\n","        N = int((depth-4)/3/2)\n","        # 4가 아닌 2를 빼는게 맞아보이긴 하는데,, 논문에서 말한 40층이 되려면 N=6에 대해 6*2*3+\"4\" 여야 40이 맞아서.. 추측컨데 projection 하는 conv도 센거 같다\n","        self.in_channels = 16\n","\n","        self.conv1 = nn.Conv2d(3, 16, 3, padding=1, bias = False)\n","        # pre-act 구조에선 첫번째 conv block에서 pool 있으면 conv-BN-relu-pool -> Bottleneck 이렇게\n","        # 이유는? 맨처음에 bn-relu를 통과시키면 데이터 전처리에서 할 일을 하게 되는 셈이다\n","        # 근데 WRN 처럼 Block 들어가기 전 pooling이 없으면? conv -> Block 으로 바로 들어가는 듯 why?\n","        # conv-bn-relu -> Block 으로 넣으면 Block 에서 bn-relu를 만나서 bn-relu-bn-relu 이렇게 돼버린다!\n","        self.conv2 = self.make_layers(16*k, N, stride = 1)\n","        self.conv3 = self.make_layers(32*k, N, stride = 2)\n","        self.conv4 = self.make_layers(64*k, N, stride = 2)\n","        self.bn = nn.BatchNorm2d(64*k)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n","        self.fc = nn.Linear(64*k, num_classes)\n","\n","        # weight initialization\n","        if init_weights:\n","            for m in self.modules():\n","                if isinstance(m, nn.Conv2d):\n","                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                    if m.bias is not None:\n","                        nn.init.constant_(m.bias, 0)\n","                elif isinstance(m, nn.BatchNorm2d):\n","                    nn.init.constant_(m.weight, 1)\n","                    nn.init.constant_(m.bias, 0)\n","                elif isinstance(m, nn.Linear):\n","                    nn.init.normal_(m.weight, 0, 0.01)\n","                    nn.init.constant_(m.bias, 0)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        x = self.avg_pool(x)\n","        x = torch.flatten(x, start_dim=1)\n","        x = self.fc(x)\n","        return x\n","\n","    def make_layers(self, out_channels, num_blocks, stride):\n","\n","        if stride != 1 or self.in_channels != out_channels:\n","            projection = nn.Conv2d(self.in_channels, out_channels, 1, stride=stride, bias = False)\n","                # nn.BatchNorm2d(inner_channels * block.expansion)) # pre-act 라서 여기선 생략\n","        else:\n","            projection = None\n","\n","        layers = []\n","        layers += [WiderBlock(self.in_channels, out_channels, stride, projection)] # projection은 첫 block에서만\n","        self.in_channels = out_channels\n","        for _ in range(1, num_blocks):\n","            layers += [WiderBlock(self.in_channels, out_channels)]\n","\n","        return nn.Sequential(*layers)"],"metadata":{"id":"GDXjvY7u9vi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = WRN(depth=28, k=10, num_classes=10)\n","# print(model)\n","!pip install torchinfo\n","from torchinfo import summary\n","summary(model, (2,3, 224, 224), device=\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7LILW56998fH","executionInfo":{"status":"ok","timestamp":1685108926771,"user_tz":-540,"elapsed":18745,"user":{"displayName":"강사","userId":"16181995035441534154"}},"outputId":"bb8b08b9-34ce-4fbd-a0cd-4e276f93be90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]},{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","WRN                                      [2, 10]                   --\n","├─Conv2d: 1-1                            [2, 16, 224, 224]         432\n","├─Sequential: 1-2                        [2, 160, 224, 224]        --\n","│    └─WiderBlock: 2-1                   [2, 160, 224, 224]        --\n","│    │    └─Sequential: 3-1              [2, 160, 224, 224]        253,792\n","│    │    └─Conv2d: 3-2                  [2, 160, 224, 224]        2,560\n","│    └─WiderBlock: 2-2                   [2, 160, 224, 224]        --\n","│    │    └─Sequential: 3-3              [2, 160, 224, 224]        461,440\n","│    └─WiderBlock: 2-3                   [2, 160, 224, 224]        --\n","│    │    └─Sequential: 3-4              [2, 160, 224, 224]        461,440\n","│    └─WiderBlock: 2-4                   [2, 160, 224, 224]        --\n","│    │    └─Sequential: 3-5              [2, 160, 224, 224]        461,440\n","├─Sequential: 1-3                        [2, 320, 112, 112]        --\n","│    └─WiderBlock: 2-5                   [2, 320, 112, 112]        --\n","│    │    └─Sequential: 3-6              [2, 320, 112, 112]        1,383,360\n","│    │    └─Conv2d: 3-7                  [2, 320, 112, 112]        51,200\n","│    └─WiderBlock: 2-6                   [2, 320, 112, 112]        --\n","│    │    └─Sequential: 3-8              [2, 320, 112, 112]        1,844,480\n","│    └─WiderBlock: 2-7                   [2, 320, 112, 112]        --\n","│    │    └─Sequential: 3-9              [2, 320, 112, 112]        1,844,480\n","│    └─WiderBlock: 2-8                   [2, 320, 112, 112]        --\n","│    │    └─Sequential: 3-10             [2, 320, 112, 112]        1,844,480\n","├─Sequential: 1-4                        [2, 640, 56, 56]          --\n","│    └─WiderBlock: 2-9                   [2, 640, 56, 56]          --\n","│    │    └─Sequential: 3-11             [2, 640, 56, 56]          5,531,520\n","│    │    └─Conv2d: 3-12                 [2, 640, 56, 56]          204,800\n","│    └─WiderBlock: 2-10                  [2, 640, 56, 56]          --\n","│    │    └─Sequential: 3-13             [2, 640, 56, 56]          7,375,360\n","│    └─WiderBlock: 2-11                  [2, 640, 56, 56]          --\n","│    │    └─Sequential: 3-14             [2, 640, 56, 56]          7,375,360\n","│    └─WiderBlock: 2-12                  [2, 640, 56, 56]          --\n","│    │    └─Sequential: 3-15             [2, 640, 56, 56]          7,375,360\n","├─BatchNorm2d: 1-5                       [2, 640, 56, 56]          1,280\n","├─ReLU: 1-6                              [2, 640, 56, 56]          --\n","├─AdaptiveAvgPool2d: 1-7                 [2, 640, 1, 1]            --\n","├─Linear: 1-8                            [2, 10]                   6,410\n","==========================================================================================\n","Total params: 36,479,194\n","Trainable params: 36,479,194\n","Non-trainable params: 0\n","Total mult-adds (G): 513.85\n","==========================================================================================\n","Input size (MB): 1.20\n","Forward/backward pass size (MB): 3847.09\n","Params size (MB): 145.92\n","Estimated Total Size (MB): 3994.22\n","=========================================================================================="]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["x = torch.randn(2,3,224,224)\n","print(model(x).shape)"],"metadata":{"id":"-TNV93Y-_wVK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685108941389,"user_tz":-540,"elapsed":14622,"user":{"displayName":"강사","userId":"16181995035441534154"}},"outputId":"99068465-ccbe-4ef7-f693-2c1b7abf9c06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 10])\n"]}]}]}