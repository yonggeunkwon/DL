{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1nrn3aHQzpks9AhG1GvrtRDmL7_12drCZ","timestamp":1677728029511}],"authorship_tag":"ABX9TyOJHne47qPd7rvjEG3OqUqs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KriMMOB2Zc2O"},"outputs":[],"source":["import torch\n","from torch import nn"]},{"cell_type":"code","source":["class DepSepConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride):\n","        super().__init__()\n","\n","        self.depthwise = nn.Sequential(nn.Conv2d(in_channels,in_channels,3, stride = stride, padding = 1, groups = in_channels, bias=False),\n","                                       nn.BatchNorm2d(in_channels),\n","                                       nn.ReLU6(inplace=True))\n","\n","        self.pointwise = nn.Sequential(nn.Conv2d(in_channels,out_channels,1, bias=False),\n","                                       nn.BatchNorm2d(out_channels))\n","                                       # no activation!!\n","    def forward(self, x):\n","        x = self.depthwise(x)\n","        x = self.pointwise(x)\n","        return x\n","\n","class InvertedBlock(nn.Module):\n","    def __init__(self, in_channels, exp_channels, out_channels, stride):\n","        super().__init__()\n","\n","        self.use_skip_connect = (stride==1 and in_channels==out_channels)\n","\n","        layers = []\n","        if in_channels != exp_channels: # 채널 안늘어날 때는 1x1 생략. 즉, 1x1은 채널을 키워야할 때만 존재한다.\n","            layers += [nn.Sequential(nn.Conv2d(in_channels, exp_channels, 1, bias=False),\n","                                     nn.BatchNorm2d(exp_channels),\n","                                     nn.ReLU6(inplace=True))]\n","        layers += [DepSepConv(exp_channels, out_channels, stride=stride)]\n","\n","        self.residual = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        if self.use_skip_connect:\n","            return x + self.residual(x) # 더하고 ReLU 하지 않는다! 그래야 linear block이 되는 거니까\n","        else:\n","            return self.residual(x)\n","\n","class MobileNetV2(nn.Module):\n","    def __init__(self, num_classes=1000):\n","        super().__init__()\n","\n","        self.configs=[# t, c, n, s\n","                      [1, 16, 1, 1],\n","                      [6, 24, 2, 2],\n","                      [6, 32, 3, 2],\n","                      [6, 64, 4, 2],\n","                      [6, 96, 3, 1],\n","                      [6, 160, 3, 2],\n","                      [6, 320, 1, 1]]\n","\n","        self.stem_conv = nn.Sequential(nn.Conv2d(3, 32, 3, padding=1, stride=2, bias=False),\n","                                       nn.BatchNorm2d(32),\n","                                       nn.ReLU6(inplace=True))\n","\n","        in_channels = 32\n","        layers = []\n","        for t, c, n, s in self.configs:\n","            for i in range(n):\n","                stride = s if i == 0 else 1\n","                exp_channels = in_channels * t\n","                layers += [InvertedBlock(in_channels=in_channels, exp_channels=exp_channels, out_channels=c, stride=stride)]\n","                in_channels = c\n","\n","        self.layers = nn.Sequential(*layers)\n","\n","        self.last_conv = nn.Sequential(nn.Conv2d(in_channels, 1280, 1, bias=False),\n","                                       nn.BatchNorm2d(1280),\n","                                       nn.ReLU6(inplace=True))\n","        \n","        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n","\n","        self.classifier = nn.Sequential(nn.Dropout(0.2), # 논문에는 상세히 나와있진 않지만 토치 문서에 있어서 포함 -> 채널 축으로 특징들이 놓여있고 그것들을 일부 가려보며 학습하는 의미\n","                                        nn.Linear(1280, num_classes))\n","\n","    def forward(self, x):\n","        x = self.stem_conv(x)\n","        x = self.layers(x)\n","        x = self.last_conv(x)\n","        x = self.avg_pool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x"],"metadata":{"id":"Oc-qxYb0ZeYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = MobileNetV2()\n","# print(model)\n","!pip install torchinfo\n","from torchinfo import summary\n","summary(model, input_size=(2,3,224,224), device='cpu')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zxYitk6oYHai","executionInfo":{"status":"ok","timestamp":1681993764080,"user_tz":-540,"elapsed":12039,"user":{"displayName":"강사","userId":"16181995035441534154"}},"outputId":"551bad8a-02f4-4388-d399-2aa991e098b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.7.2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  action_fn=lambda data: sys.getsizeof(data.storage()),\n","/usr/local/lib/python3.9/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return super().__sizeof__() + self.nbytes()\n"]},{"output_type":"execute_result","data":{"text/plain":["====================================================================================================\n","Layer (type:depth-idx)                             Output Shape              Param #\n","====================================================================================================\n","MobileNetV2                                        [2, 1000]                 --\n","├─Sequential: 1-1                                  [2, 32, 112, 112]         --\n","│    └─Conv2d: 2-1                                 [2, 32, 112, 112]         864\n","│    └─BatchNorm2d: 2-2                            [2, 32, 112, 112]         64\n","│    └─ReLU6: 2-3                                  [2, 32, 112, 112]         --\n","├─Sequential: 1-2                                  [2, 320, 7, 7]            --\n","│    └─InvertedBlock: 2-4                          [2, 16, 112, 112]         --\n","│    │    └─Sequential: 3-1                        [2, 16, 112, 112]         896\n","│    └─InvertedBlock: 2-5                          [2, 24, 56, 56]           --\n","│    │    └─Sequential: 3-2                        [2, 24, 56, 56]           5,136\n","│    └─InvertedBlock: 2-6                          [2, 24, 56, 56]           --\n","│    │    └─Sequential: 3-3                        [2, 24, 56, 56]           8,832\n","│    └─InvertedBlock: 2-7                          [2, 32, 28, 28]           --\n","│    │    └─Sequential: 3-4                        [2, 32, 28, 28]           10,000\n","│    └─InvertedBlock: 2-8                          [2, 32, 28, 28]           --\n","│    │    └─Sequential: 3-5                        [2, 32, 28, 28]           14,848\n","│    └─InvertedBlock: 2-9                          [2, 32, 28, 28]           --\n","│    │    └─Sequential: 3-6                        [2, 32, 28, 28]           14,848\n","│    └─InvertedBlock: 2-10                         [2, 64, 14, 14]           --\n","│    │    └─Sequential: 3-7                        [2, 64, 14, 14]           21,056\n","│    └─InvertedBlock: 2-11                         [2, 64, 14, 14]           --\n","│    │    └─Sequential: 3-8                        [2, 64, 14, 14]           54,272\n","│    └─InvertedBlock: 2-12                         [2, 64, 14, 14]           --\n","│    │    └─Sequential: 3-9                        [2, 64, 14, 14]           54,272\n","│    └─InvertedBlock: 2-13                         [2, 64, 14, 14]           --\n","│    │    └─Sequential: 3-10                       [2, 64, 14, 14]           54,272\n","│    └─InvertedBlock: 2-14                         [2, 96, 14, 14]           --\n","│    │    └─Sequential: 3-11                       [2, 96, 14, 14]           66,624\n","│    └─InvertedBlock: 2-15                         [2, 96, 14, 14]           --\n","│    │    └─Sequential: 3-12                       [2, 96, 14, 14]           118,272\n","│    └─InvertedBlock: 2-16                         [2, 96, 14, 14]           --\n","│    │    └─Sequential: 3-13                       [2, 96, 14, 14]           118,272\n","│    └─InvertedBlock: 2-17                         [2, 160, 7, 7]            --\n","│    │    └─Sequential: 3-14                       [2, 160, 7, 7]            155,264\n","│    └─InvertedBlock: 2-18                         [2, 160, 7, 7]            --\n","│    │    └─Sequential: 3-15                       [2, 160, 7, 7]            320,000\n","│    └─InvertedBlock: 2-19                         [2, 160, 7, 7]            --\n","│    │    └─Sequential: 3-16                       [2, 160, 7, 7]            320,000\n","│    └─InvertedBlock: 2-20                         [2, 320, 7, 7]            --\n","│    │    └─Sequential: 3-17                       [2, 320, 7, 7]            473,920\n","├─Sequential: 1-3                                  [2, 1280, 7, 7]           --\n","│    └─Conv2d: 2-21                                [2, 1280, 7, 7]           409,600\n","│    └─BatchNorm2d: 2-22                           [2, 1280, 7, 7]           2,560\n","│    └─ReLU6: 2-23                                 [2, 1280, 7, 7]           --\n","├─AdaptiveAvgPool2d: 1-4                           [2, 1280, 1, 1]           --\n","├─Sequential: 1-5                                  [2, 1000]                 --\n","│    └─Dropout: 2-24                               [2, 1280]                 --\n","│    └─Linear: 2-25                                [2, 1000]                 1,281,000\n","====================================================================================================\n","Total params: 3,504,872\n","Trainable params: 3,504,872\n","Non-trainable params: 0\n","Total mult-adds (M): 601.62\n","====================================================================================================\n","Input size (MB): 1.20\n","Forward/backward pass size (MB): 213.72\n","Params size (MB): 14.02\n","Estimated Total Size (MB): 228.94\n","===================================================================================================="]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["x = torch.randn(2,3,224,224)\n","print(model(x).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s5YOyHfdprL7","executionInfo":{"status":"ok","timestamp":1681993764080,"user_tz":-540,"elapsed":8,"user":{"displayName":"강사","userId":"16181995035441534154"}},"outputId":"33243e0c-9c28-4fc9-c4f9-e6f8566d69cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 1000])\n"]}]}]}