{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 5])\n",
      "Parameter containing:\n",
      "tensor([[-0.3502,  0.2985,  0.0194],\n",
      "        [-0.4961, -0.1091,  0.2607],\n",
      "        [ 0.0743, -0.5648, -0.5665],\n",
      "        [-0.2612,  0.3939, -0.2900],\n",
      "        [ 0.0668, -0.5712,  0.0923]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0614, -0.2793,  0.4595, -0.5550,  0.5238], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(100, 3)\n",
    "layer = nn.Linear(3, 5)\n",
    "print(layer(x).shape)\n",
    "print(layer.weight)\n",
    "print(layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4707, 0.0000, 0.0000, 0.4578, 0.0000],\n",
      "        [2.0292, 0.0779, 0.0000, 0.4548, 0.5102]])\n"
     ]
    }
   ],
   "source": [
    "# ReLU 통과시키면 음수가 모두 사라짐\n",
    "x = torch.randn(2, 5)\n",
    "layer = nn.ReLU()\n",
    "print(layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, -0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 0.0000],\n",
      "        [0.8667, -0.0000, 0.0000, -0.0000, 0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, 0.0000, -0.0000, -0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# dropout에서 p는 죽일 확률\n",
    "x = torch.randn(3, 7)\n",
    "drop = nn.Dropout(p = 0.9)\n",
    "print(drop(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000, -0.4967, -0.6252, -0.0000,  0.0000, -0.0000,  0.1559],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  1.3528],\n",
      "        [ 0.9132, -0.7828, -0.0988,  0.2376,  0.1345,  0.1172, -0.2609]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0954, -0.3477, -0.4376, -0.2687,  0.3355, -0.9282,  0.1092],\n",
      "        [-0.2191, -1.0686, -0.0944, -0.4416,  0.2446, -0.8632,  0.9469],\n",
      "        [ 0.6392, -0.5479, -0.0692,  0.1663,  0.0942,  0.0820, -0.1826]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class sample_model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_layer = nn.Sequential(nn.Linear(5, 7),\n",
    "                                        nn.Dropout(p=0.3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop_layer(x)\n",
    "        return x\n",
    "    \n",
    "model = sample_model()\n",
    "model.train()\n",
    "x = torch.randn(3, 5)\n",
    "print(model(x))\n",
    "\n",
    "# test mode에서는 살아남을 확률(0.7)이 원래 값에 곱해져서 나간다\n",
    "model.eval()\n",
    "print(model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 4, 4])\n",
      "torch.Size([7, 6, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Conv2d(6, 7, 4, stride=1, padding=1)\n",
    "print(layer(torch.randn(32, 6, 5, 5)).shape)\n",
    "print(layer.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 12, 12])\n",
      "torch.Size([32, 16, 12, 12])\n",
      "torch.Size([32, 16, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(1, 8, 6, stride=2)\n",
    "x = torch.randn(32, 1, 28, 28)\n",
    "print(conv1(x).shape)\n",
    "\n",
    "conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "print(conv2(conv1(x)).shape)\n",
    "\n",
    "Maxpool = nn.MaxPool2d(kernel_size=2, stride=(2, 2))\n",
    "print(Maxpool(conv2(conv1(x))).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 2.6342,  0.8048,  0.2087,  0.8149, -0.1825, -0.6585],\n",
      "          [ 1.0300, -0.4161, -0.3111, -0.9790,  0.8363,  0.6256],\n",
      "          [-0.4819,  2.4960,  0.3705,  0.5296, -1.7951,  1.4587],\n",
      "          [ 0.5513,  0.7049,  0.1465,  0.9725, -0.2986, -0.4650],\n",
      "          [ 0.1393, -0.9076, -0.1938,  0.7208,  0.0054, -1.4444],\n",
      "          [ 0.9417,  0.2168,  0.0344, -1.1267, -0.1688,  1.1078]]]])\n",
      "tensor([[[[2.6342, 0.8149, 0.8363],\n",
      "          [2.4960, 0.9725, 1.4587],\n",
      "          [0.9417, 0.7208, 1.1078]]]])\n"
     ]
    }
   ],
   "source": [
    "maxpool = nn.MaxPool2d(2) # 2로만 줘도 자동으로 kernel_size=2, stride=(2, 2)\n",
    "x = torch.randn(1, 1, 6, 6)\n",
    "print(x)\n",
    "print(maxpool(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.3224e-01,  1.3153e+00,  1.1555e+00, -3.1420e-01,  1.4108e+00,\n",
      "            8.5479e-01],\n",
      "          [ 7.5792e-01,  1.0340e+00,  9.5334e-01,  2.8559e+00,  2.2333e-01,\n",
      "            2.6221e-01],\n",
      "          [-2.6416e-01, -3.8056e-01,  5.9388e-01, -2.5561e-01,  1.0151e+00,\n",
      "            4.6531e-01],\n",
      "          [ 9.9889e-02, -8.6791e-01, -5.6894e-01, -1.1764e+00,  1.9463e+00,\n",
      "           -4.1642e-01],\n",
      "          [-1.7762e+00,  1.4012e+00, -1.1229e+00,  4.1898e-01,  2.9276e-01,\n",
      "            1.0660e+00],\n",
      "          [ 8.8256e-01, -1.2590e-01, -1.7291e-01, -8.0863e-01,  2.7510e-03,\n",
      "           -8.6627e-01]]]])\n",
      "tensor([[[[ 0.8099,  1.1626,  0.6878],\n",
      "          [-0.3532, -0.3518,  0.7526],\n",
      "          [ 0.0954, -0.4214,  0.1238]]]])\n",
      "torch.Size([32, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "avgpool = nn.AvgPool2d(2)\n",
    "x = torch.randn(1, 1, 6, 6)\n",
    "print(x)\n",
    "print(avgpool(x))\n",
    "print(avgpool(torch.randn(32, 3, 6, 6)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 8, 3, padding=1),\n",
    "                                   nn.BatchNorm2d(8),\n",
    "                                   nn.ReLU())\n",
    "        self.Maxpool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(8, 16, 3, padding=1),\n",
    "                                   nn.BatchNorm2d(16),\n",
    "                                   nn.ReLU())\n",
    "        self.Maxpool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(16, 32, 3, padding=1),\n",
    "                                   nn.BatchNorm2d(32),\n",
    "                                   nn.ReLU())\n",
    "        self.Maxpool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc = nn.Linear(32*4*4, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.Maxpool1(x)        \n",
    "        x = self.conv2(x)\n",
    "        x = self.Maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.Maxpool3(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn(32, 3, 32, 32)\n",
    "model = CNN()\n",
    "print(model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .parameters() vs .modules() vs .children() 그리고 isinstance의 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n",
      "MLP(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=3, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=4, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc_out): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(2,3),\n",
    "                                 nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(3,4),\n",
    "                                 nn.ReLU())\n",
    "        self.fc_out = nn.Sequential(nn.Linear(4,1),\n",
    "                                    nn.Sigmoid())\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "        \n",
    "model = MLP()\n",
    "print(model(torch.randn(2,2)).shape)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fef7d4fbd60>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4047, -0.4735],\n",
       "        [-0.1519, -0.0417],\n",
       "        [-0.3605,  0.4823]], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0]\n",
    "# [layer0 weight 값, layer0 bias 값, layer1 weight 값, layer1 bias 값, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.0650, -0.1084,  0.1067,  0.1666],\n",
      "        [ 0.0571, -0.0531,  0.0947, -0.4879],\n",
      "        [ 0.4764, -0.4066, -0.0661,  0.0385],\n",
      "        [ 0.1560, -0.2527,  0.3926,  0.0518],\n",
      "        [-0.1650, -0.3495, -0.1433,  0.3744],\n",
      "        [-0.1757,  0.0883, -0.0162,  0.3376],\n",
      "        [ 0.0062, -0.2141, -0.0682,  0.0047],\n",
      "        [-0.3425, -0.3409, -0.4371,  0.4798],\n",
      "        [-0.1241, -0.4228,  0.2612,  0.2149],\n",
      "        [ 0.2722,  0.4271, -0.3821,  0.3314]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0477, -0.2718, -0.4806, -0.2161, -0.4872,  0.1474,  0.3046, -0.3244,\n",
      "        -0.3586,  0.2718], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# for transfer learning\n",
    "model = MLP()\n",
    "# print([p for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "for p in model.parameters(): # 전체 freeze\n",
    "    p.requires_grad = False\n",
    "model.fc_out = nn.Linear(4, 10)\n",
    "\n",
    "# 모두 얼렸기 때문에 빈 list가 나온다\n",
    "# print([p for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "params = ([p for p in model.parameters() if p.requires_grad])\n",
    "print(params)\n",
    "\n",
    "# requires_grad가 true인 것들만 학습된다 \n",
    "from torch import optim\n",
    "optimizer = optim.Adam(params, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.0.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.6263,  0.6195],\n",
      "        [ 0.1901, -0.1824],\n",
      "        [-0.0325,  0.3171]])\n",
      "fc1.0.bias\n",
      "Parameter containing:\n",
      "tensor([-0.5685,  0.2003,  0.2153])\n",
      "fc2.0.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3623,  0.5447,  0.5735],\n",
      "        [ 0.0638,  0.1563,  0.0239],\n",
      "        [-0.4210, -0.2674, -0.5510],\n",
      "        [-0.1019, -0.0227,  0.0784]])\n",
      "fc2.0.bias\n",
      "Parameter containing:\n",
      "tensor([-0.4456, -0.3449,  0.5631, -0.1026])\n",
      "fc_out.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0650, -0.1084,  0.1067,  0.1666],\n",
      "        [ 0.0571, -0.0531,  0.0947, -0.4879],\n",
      "        [ 0.4764, -0.4066, -0.0661,  0.0385],\n",
      "        [ 0.1560, -0.2527,  0.3926,  0.0518],\n",
      "        [-0.1650, -0.3495, -0.1433,  0.3744],\n",
      "        [-0.1757,  0.0883, -0.0162,  0.3376],\n",
      "        [ 0.0062, -0.2141, -0.0682,  0.0047],\n",
      "        [-0.3425, -0.3409, -0.4371,  0.4798],\n",
      "        [-0.1241, -0.4228,  0.2612,  0.2149],\n",
      "        [ 0.2722,  0.4271, -0.3821,  0.3314]], requires_grad=True)\n",
      "fc_out.bias\n",
      "Parameter containing:\n",
      "tensor([-0.0477, -0.2718, -0.4806, -0.2161, -0.4872,  0.1474,  0.3046, -0.3244,\n",
      "        -0.3586,  0.2718], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    print(name)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.modules at 0x7fef7ce5a200>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MLP(\n",
       "   (fc1): Sequential(\n",
       "     (0): Linear(in_features=2, out_features=3, bias=True)\n",
       "     (1): ReLU()\n",
       "   )\n",
       "   (fc2): Sequential(\n",
       "     (0): Linear(in_features=3, out_features=4, bias=True)\n",
       "     (1): ReLU()\n",
       "   )\n",
       "   (fc_out): Linear(in_features=4, out_features=10, bias=True)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Linear(in_features=2, out_features=3, bias=True)\n",
       "   (1): ReLU()\n",
       " ),\n",
       " Linear(in_features=2, out_features=3, bias=True),\n",
       " ReLU(),\n",
       " Sequential(\n",
       "   (0): Linear(in_features=3, out_features=4, bias=True)\n",
       "   (1): ReLU()\n",
       " ),\n",
       " Linear(in_features=3, out_features=4, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=4, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear(in_features=2, out_features=3, bias=True), Linear(in_features=3, out_features=4, bias=True), Linear(in_features=4, out_features=10, bias=True)]\n",
      "[Parameter containing:\n",
      "tensor([[-0.6263,  0.6195],\n",
      "        [ 0.1901, -0.1824],\n",
      "        [-0.0325,  0.3171]]), Parameter containing:\n",
      "tensor([[-0.3623,  0.5447,  0.5735],\n",
      "        [ 0.0638,  0.1563,  0.0239],\n",
      "        [-0.4210, -0.2674, -0.5510],\n",
      "        [-0.1019, -0.0227,  0.0784]]), Parameter containing:\n",
      "tensor([[-0.0650, -0.1084,  0.1067,  0.1666],\n",
      "        [ 0.0571, -0.0531,  0.0947, -0.4879],\n",
      "        [ 0.4764, -0.4066, -0.0661,  0.0385],\n",
      "        [ 0.1560, -0.2527,  0.3926,  0.0518],\n",
      "        [-0.1650, -0.3495, -0.1433,  0.3744],\n",
      "        [-0.1757,  0.0883, -0.0162,  0.3376],\n",
      "        [ 0.0062, -0.2141, -0.0682,  0.0047],\n",
      "        [-0.3425, -0.3409, -0.4371,  0.4798],\n",
      "        [-0.1241, -0.4228,  0.2612,  0.2149],\n",
      "        [ 0.2722,  0.4271, -0.3821,  0.3314]], requires_grad=True)]\n",
      "[None, None, None]\n"
     ]
    }
   ],
   "source": [
    "# gradient vanishing 일어나고 있는지도 확인 가능 (그림 그려서)\n",
    "print([m for m in model.modules() if isinstance(m,nn.Linear)])\n",
    "print([m.weight for m in model.modules() if isinstance(m,nn.Linear)])\n",
    "print([m.weight.grad for m in model.modules() if isinstance(m,nn.Linear)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]]), Parameter containing:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]), Parameter containing:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# weight initialization에 활용\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "\n",
    "print([m.weight for m in model.modules() if isinstance(m, nn.Linear)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.children at 0x7fef7ce5a900>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): Linear(in_features=2, out_features=3, bias=True)\n",
       "   (1): ReLU()\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Linear(in_features=3, out_features=4, bias=True)\n",
       "   (1): ReLU()\n",
       " ),\n",
       " Linear(in_features=4, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000],\n",
       "        [0.2814, 1.0502, 1.0652]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,2)\n",
    "list(model.children())[0](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      ") Sequential(\n",
      "  (0): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=3, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=4, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "tensor([[0.0000, 0.0000, 0.5631, 0.0000],\n",
      "        [1.9512, 2.0519, 2.9599, 2.2943]])\n"
     ]
    }
   ],
   "source": [
    "print(*list(model.children())[:2])\n",
    "sub_network = nn.Sequential(*list(model.children())[:2])\n",
    "print(sub_network)\n",
    "print(sub_network(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModuleList vs Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (1): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (2): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (3): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (4): Linear(in_features=3, out_features=3, bias=True)\n",
      ")\n",
      "ModuleList(\n",
      "  (0-4): 5 x Linear(in_features=3, out_features=3, bias=True)\n",
      ")\n",
      "tensor([[-0.1562, -0.3351, -0.3850]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1562, -0.3351, -0.3850]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "fc=nn.Linear(3,3)\n",
    "layer_list = [fc for _ in range(5)]\n",
    "layers1 = nn.Sequential(*layer_list)\n",
    "layers2 = nn.ModuleList(layer_list)\n",
    "print(layers1)\n",
    "print(layers2)\n",
    "\n",
    "x=torch.randn(1,3)\n",
    "print(layers1(x))\n",
    "\n",
    "# print(layers2(x)) # error!\n",
    "for layer in layers2:\n",
    "    x = layer(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2951, -0.0468,  0.2756]], grad_fn=<AddmmBackward0>)\n",
      "testNet(\n",
      "  (Module_List): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=3, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 걍 리스트 쓰지 왜 nn.ModuleList 를 쓸까?\n",
    "class testNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.Module_List = [nn.Linear(3,3), nn.Linear(3,3)]\n",
    "        self.Module_List = nn.ModuleList([nn.Linear(3,3), nn.Linear(3,3)])\n",
    "\n",
    "    def forward(self,x):\n",
    "        for layer in self.Module_List:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model=testNet()\n",
    "print(model(torch.randn(1,3)))\n",
    "\n",
    "print(model) # 그냥 리스트로 하면 등록이 안돼있다!\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.1) # 등록이 안돼있으면 parameter를 못 찾는다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_block(\n",
      "  (block_x): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (block_y): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): small_block(\n",
      "    (block_x): Linear(in_features=1, out_features=1, bias=True)\n",
      "    (block_y): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      "  (1): small_block(\n",
      "    (block_x): Linear(in_features=1, out_features=1, bias=True)\n",
      "    (block_y): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([-1.2391], grad_fn=<AddBackward0>) tensor([-0.7862], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 그럼 nn.Sequential 쓰고 말지 왜 굳이 nn.ModuleList?\n",
    "class small_block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block_x = nn.Linear(1,1)\n",
    "        self.block_y = nn.Linear(1,1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.block_x(x)\n",
    "        y = self.block_y(y)\n",
    "        return x, y\n",
    "\n",
    "block = small_block()\n",
    "print(block)\n",
    "model = nn.Sequential(block, block)\n",
    "print(model)\n",
    "# model(torch.randn(1), torch.randn(1)) # error!\n",
    "# nn.Sequential 이 가지고 있는 forward 함수를 call 하기 때문에 입력을 두 개 넣으면 안된다!!\n",
    "\n",
    "model = nn.ModuleList([block,block])\n",
    "x = torch.randn(1)\n",
    "y = torch.randn(1)\n",
    "for block in model:\n",
    "    x, y = block(x,y)\n",
    "print(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- modulelist는 forward 함수가 정의되어 있지 않다.\n",
    "- sequential에는 하나를 입력받아서 하나를 출력하는 forward 함수가 정의되어 있다.\n",
    "- 그래서 경우에 맞게 사용하면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
